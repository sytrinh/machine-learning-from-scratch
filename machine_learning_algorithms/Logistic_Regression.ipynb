{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic Regression",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMXpLx6+jPdn8VnHstFYuXI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sytrinh/machine-learning-from-scratch/blob/main/machine_learning_algorithms/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "Logistic regression is not a regression problem but a classification problem (binary classification for specific), in which the outcome only has two unique values (usually 0 and 1).\n",
        "\n",
        "In linear regression, we generally need to find a function $\\mathbf{f}$ so that:\n",
        "\n",
        "$$y = f(\\mathbf{w}^T \\mathbf{x}),$$\n",
        "where $\\mathbf{x}$ and $\\mathbf{w}$ are column vectors. This is not appropriate for a binary classification problem because of two reasons:\n",
        "- First, linear regression deals with continuous values whereas classification problems mandate discrete values.\n",
        "- Second, when $\\mathbf{x}$ is large, the predicted value $\\hat{y}$ is far more larger than $1$; and when $\\mathbf{x}$ is small, the predicted value $\\hat{y}$ can be negative. These two cases do not make sense for a binary classification problem.\n",
        "\n",
        "## Logistic regression model\n",
        "\n",
        "### One data point\n",
        "\n",
        "In logistic regression, instead of predicting the outcome directly, we model to predict the probability of the outcome equal to 1 or 0. \n",
        "\n",
        "$$P(y_i=1 | \\mathbf{x}_i; \\mathbf{w}) = f(\\mathbf{w}^T \\mathbf{x})$$\n",
        "\n",
        "$$P(y_i=0 | \\mathbf{x}_i; \\mathbf{w}) = 1 - f(\\mathbf{w}^T \\mathbf{x}),$$\n",
        "\n",
        "where $P(y_i=1 | \\mathbf{x}_i; \\mathbf{w})$ is the probability that $y_i=1$ given the model parameters $\\mathbf{w}$ and the data $\\mathbf{x}_i$, and $P(y_i=0 | \\mathbf{x}_i; \\mathbf{w})$ is the probability that $y_i=0$ given the model parameters $\\mathbf{w}$ and the data $\\mathbf{x}_i$. Note that $\\mathbf{x}_i$ and $\\mathbf{y}_i$ are random variables of one data point. $\\mathbf{x}_i$ contains values of all features in that data point, and $\\mathbf{y}_i$ is the outcome.\n",
        "\n",
        "These probabilities can be combined in only one formula:\n",
        "\n",
        "$$P(y_i | \\mathbf{x}_i; \\mathbf{w}) = z_i^{y_i} (1-z_i)^{1-y_i},$$\n",
        "where $z_i = f(\\mathbf{w}^T \\mathbf{x})$\n",
        "\n",
        "### Whole training set\n",
        "\n",
        "- $\\mathbf{X} = [\\mathbf{x}_1, \\mathbf{x}_2, ... , \\mathbf{x}_M]^T$: a $M \\times N$ matrix containing all feature values.\n",
        "- $\\mathbf{y} = [y_1, y_2, ... , y_M]^T$: an $M$-dimensional vector containing all outcome values.\n",
        "\n",
        "The probability of $\\mathbf{y}$ given $\\mathbf{X}$ is:\n",
        "\n",
        "$$P (\\mathbf{y} | \\mathbf{X}; \\mathbf{w})$$\n",
        "\n",
        "To be likely to receive the outcomes $\\mathbf{y}$ in reality, this probability must be large. Therefore, we need to find $\\mathbf{w}$ that maximize this probability.\n",
        "\n",
        "$$\\mathbf{w} = \\arg\\max_{\\mathbf{w}} P(\\mathbf{y}|\\mathbf{X}; \\mathbf{w})$$\n",
        "\n",
        "This problem, which finds the model parameters so that the model produces outcomes closest to the data, is called **maximum likelihood estimation** (MLE).\n",
        "\n",
        "### Assumption\n",
        "\n",
        "To solve this problem, we need to assume that the data points are independent of each other. it means that $y_i$ is independent of $y_j$ with $i \\neq j$, and $y_i$ is also independent of $x_j$. Then,\n",
        "\n",
        "$$P (\\mathbf{y} | \\mathbf{X}; \\mathbf{w}) = \\prod_{i=1}^M P(y_i| \\mathbf{x}_i; \\mathbf{w}) = \\prod_{i=1}^M z_i^{y_i}(1 - z_i)^{1- y_i}$$\n",
        "\n",
        "To find the maximum of this probability, we can instead find the minumum of the following function:\n",
        "\n",
        "$$J(\\mathbf{w}) = - \\frac{1}{M} \\log P(\\mathbf{y}|\\mathbf{X}; \\mathbf{w}) = -\\frac{1}{M} \\sum_{i=1}^M(y_i \\log {z}_i + (1-y_i) \\log (1 - {z}_i)),$$\n",
        "\n",
        "where $J(\\mathbf{w})$ is the cost function, and the formula on the right hand side is called **cross entropy**, which is often used to measure distance between two distribution. We can vectorize as follows:\n",
        "\n",
        "$$J(\\mathbf{w}) = - \\frac{1}{M} \\log P(\\mathbf{y}|\\mathbf{X}; \\mathbf{w}) = - \\frac{1}{M} \\Big(-\\mathbf{y}^T \\log \\mathbf{z} - (1-\\mathbf{y})^T \\log (1-\\mathbf{z}) \\Big)$$\n",
        "\n",
        "where $\\mathbf{z} = f(\\mathbf{X} \\mathbf{w})$\n",
        "\n",
        "### Sigmoid function\n",
        "\n",
        "Another assumption in logistic regression is that the function $f$ is assumed to be the **sigmoid** function: \n",
        "\n",
        "$$f(s) = \\frac{1}{1 + e^{-s}} \\triangleq \\sigma(s)$$\n",
        "\n",
        "This function is bounded by 0 and 1, and takes values in $(0, 1)$, so it is suitable for the binary classification problem.\n",
        "\n",
        "### Minimizing the loss function\n",
        "\n",
        "To minimizing the loss function, we use the Gradient Descent algorithm. \n",
        "\n",
        "Using the sigmoid function, we have:\n",
        "\n",
        "$$\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}} = \\frac{1}{M} \\mathbf{X}^T (\\mathbf{z}-\\mathbf{y})$$\n",
        "\n",
        "and update step is:\n",
        "\n",
        "$$\\mathbf{w} := \\mathbf{w} - \\alpha \\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}}$$\n",
        "\n",
        "Note that when considering bias, we have: $\\mathbf{z} = f(\\mathbf{X} \\mathbf{w} + b)$. We can easily find the gradient respect to $b$ and so the update formula.\n"
      ],
      "metadata": {
        "id": "ssFLIuVkiDIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation"
      ],
      "metadata": {
        "id": "wAeQR_W0_seJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "dataset = load_breast_cancer(as_frame=True)"
      ],
      "metadata": {
        "id": "RrynJWan_ubS"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the data\n",
        "dataset.data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "5ml5LV_aAb8c",
        "outputId": "66602f01-1841-4677-b2dd-6acf5564eb64"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
              "0        17.99         10.38          122.80     1001.0          0.11840   \n",
              "1        20.57         17.77          132.90     1326.0          0.08474   \n",
              "2        19.69         21.25          130.00     1203.0          0.10960   \n",
              "3        11.42         20.38           77.58      386.1          0.14250   \n",
              "4        20.29         14.34          135.10     1297.0          0.10030   \n",
              "\n",
              "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
              "0           0.27760          0.3001              0.14710         0.2419   \n",
              "1           0.07864          0.0869              0.07017         0.1812   \n",
              "2           0.15990          0.1974              0.12790         0.2069   \n",
              "3           0.28390          0.2414              0.10520         0.2597   \n",
              "4           0.13280          0.1980              0.10430         0.1809   \n",
              "\n",
              "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
              "0                 0.07871  ...         25.38          17.33           184.60   \n",
              "1                 0.05667  ...         24.99          23.41           158.80   \n",
              "2                 0.05999  ...         23.57          25.53           152.50   \n",
              "3                 0.09744  ...         14.91          26.50            98.87   \n",
              "4                 0.05883  ...         22.54          16.67           152.20   \n",
              "\n",
              "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
              "0      2019.0            0.1622             0.6656           0.7119   \n",
              "1      1956.0            0.1238             0.1866           0.2416   \n",
              "2      1709.0            0.1444             0.4245           0.4504   \n",
              "3       567.7            0.2098             0.8663           0.6869   \n",
              "4      1575.0            0.1374             0.2050           0.4000   \n",
              "\n",
              "   worst concave points  worst symmetry  worst fractal dimension  \n",
              "0                0.2654          0.4601                  0.11890  \n",
              "1                0.1860          0.2750                  0.08902  \n",
              "2                0.2430          0.3613                  0.08758  \n",
              "3                0.2575          0.6638                  0.17300  \n",
              "4                0.1625          0.2364                  0.07678  \n",
              "\n",
              "[5 rows x 30 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cc85a3ab-dd14-4ee8-8230-a4cbd3d931fd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean radius</th>\n",
              "      <th>mean texture</th>\n",
              "      <th>mean perimeter</th>\n",
              "      <th>mean area</th>\n",
              "      <th>mean smoothness</th>\n",
              "      <th>mean compactness</th>\n",
              "      <th>mean concavity</th>\n",
              "      <th>mean concave points</th>\n",
              "      <th>mean symmetry</th>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <th>...</th>\n",
              "      <th>worst radius</th>\n",
              "      <th>worst texture</th>\n",
              "      <th>worst perimeter</th>\n",
              "      <th>worst area</th>\n",
              "      <th>worst smoothness</th>\n",
              "      <th>worst compactness</th>\n",
              "      <th>worst concavity</th>\n",
              "      <th>worst concave points</th>\n",
              "      <th>worst symmetry</th>\n",
              "      <th>worst fractal dimension</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>...</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>...</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>...</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>...</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>...</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 30 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cc85a3ab-dd14-4ee8-8230-a4cbd3d931fd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cc85a3ab-dd14-4ee8-8230-a4cbd3d931fd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cc85a3ab-dd14-4ee8-8230-a4cbd3d931fd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrZz8vxbALnX",
        "outputId": "bc7a28c2-6057-4920-a0a0-2dfd58d6eb80"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      0\n",
              "1      0\n",
              "2      0\n",
              "3      0\n",
              "4      0\n",
              "      ..\n",
              "564    0\n",
              "565    0\n",
              "566    0\n",
              "567    0\n",
              "568    1\n",
              "Name: target, Length: 569, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lavctLwaCFns",
        "outputId": "f2092116-130d-48fc-f0ce-8ed9761af2f6"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset.data.to_numpy()\n",
        "target = dataset.target.to_numpy()\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, target, train_size=0.8, shuffle=True, random_state=1) \n",
        "\n",
        "# Normalizing the data\n",
        "mean = np.mean(X_train, axis=0)\n",
        "std = np.std(X_train, axis=0)\n",
        "X_train = (X_train - mean)/std\n",
        "X_test = (X_test - mean)/std"
      ],
      "metadata": {
        "id": "InFd4atZBl8I"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyLogR:\n",
        "\n",
        "  def __init__(self, lr=0.01, n_iters=5000):\n",
        "    self.lr = lr\n",
        "    self.n_iters = n_iters\n",
        "    self.params = {\n",
        "        'W': None,\n",
        "        'b': None\n",
        "    }\n",
        "\n",
        "  def sigmoid(self, s):\n",
        "    return 1/(1 + np.exp(-s))\n",
        "  \n",
        "  def init_params(self, n_features):\n",
        "    self.params['W'] = np.random.randn(n_features, 1)\n",
        "    self.params['b'] = 0\n",
        "\n",
        "  def calculate_probs(self, X):\n",
        "    W, b = self.params['W'], self.params['b']\n",
        "    a = X @ W\n",
        "    z = self.sigmoid(X @ W + b)\n",
        "    return z\n",
        "\n",
        "  def gradient_descent(self, X, z, y):\n",
        "    M = X.shape[0]\n",
        "    W, b = self.params['W'], self.params['b']\n",
        "    dW = 1/M * X.T @ (z-y)\n",
        "    db = 1/M * np.sum(z-y)\n",
        "    W = W - self.lr*dW\n",
        "    b = b - self.lr*db\n",
        "    self.params['W'] = W\n",
        "    self.params['b'] = b\n",
        "\n",
        "  def train(self, X_train, y_train):\n",
        "    X = np.asarray(X_train).copy()\n",
        "    y = np.asarray(y_train).reshape((-1,1))\n",
        "    assert X.shape[0] == y.shape[0]\n",
        "    \n",
        "    M, N = X.shape\n",
        "    self.init_params(N)\n",
        "    for i in range(self.n_iters):\n",
        "      z = self.calculate_probs(X)\n",
        "      self.gradient_descent(X, z, y)\n",
        "\n",
        "  def predict(self, X_test):\n",
        "    W, b = self.params['W'], self.params['b']\n",
        "    probs = self.sigmoid(X_test @ W + b)\n",
        "    return probs.flatten()\n",
        "\n",
        "\n",
        "mylogr = MyLogR()\n",
        "mylogr.train(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = (mylogr.predict(X_test) > 0.5)*1\n",
        "mylogr_accuracy = np.sum(y_pred == y_test)/len(y_test)\n",
        "mylogr_accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1C9-PNffCnD8",
        "outputId": "49b56784-229a-4c51-beee-860768fff0e2"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9736842105263158"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using sklearn\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression \n",
        "logR = LogisticRegression()\n",
        "logR.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "sk_y_pred = logR.predict(X_test)\n",
        "sk_accuracy = np.sum(sk_y_pred == y_test)/len(y_test)\n",
        "sk_accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJoRvOmxMiHf",
        "outputId": "6d761b18-9123-47da-f7f9-7397e9807f2e"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9736842105263158"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"My Implementation: {mylogr_accuracy}\\nSklearn Implementation: {sk_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peC6FDKUVDOn",
        "outputId": "7cce8274-9d87-4cde-fe29-1db834d3a004"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My Implementation: 0.9736842105263158\n",
            "Sklearn Implementation: 0.9736842105263158\n"
          ]
        }
      ]
    }
  ]
}