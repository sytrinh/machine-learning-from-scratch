{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear Regression",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPryBBt7sDEHfDXxqv4bFJc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sytrinh/machine-learning-from-scratch/blob/main/machine_learning_algorithms/Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression Problem\n",
        "\n",
        "Let's assume that we want to predict $y$ from a list of independent variables $x_1, x_2, ..., x_n$, and their relationship can be expressed by a function $f_t$ as\n",
        "\n",
        "$$y = f_t(\\mathbf{x}),$$\n",
        "\n",
        "where $\\mathbf{x} = [x_1, x_2, ..., x_N]$ is a row vector. The true function $f_t$ is generally unknown, but we consider a function $f$ that is approximately $f_t$ that can generate predictions close to the grounth truth ($y$), given by \n",
        "\n",
        "$$y \\approx \\hat{y} = f(\\mathbf{x}) = \\mathbf{x} \\mathbf{w},$$\n",
        "\n",
        "where $\\mathbf{w} = [w_1, w_2, ..., w_N]^T$ is a column vector of model parameters. \n",
        "\n",
        "If we want to account for the bias, $\\mathbf{x}$ and $\\mathbf{w}$ are given as follows:\n",
        "\n",
        "$$\\mathbf{x} = [1, x_1, x_2, ..., x_N]$$\n",
        "$$\\mathbf{w} = [w_0, w_1, w_2, ..., w_N]^T$$\n",
        "\n",
        "### Data\n",
        "\n",
        "Now assume that each predictor $x_i$ has $M$ samples, then all the values of $N$ features can be put into a $M \\times N$ matrix $\\mathbf{X}$.\n",
        "\n",
        "$$\\mathbf{X} = [\\mathbf{1}, \\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_N],$$\n",
        "where $\\mathbf{x}_i$ is an $M$-dimensional column vector containing $M$ samples of predictor $x_i$.\n",
        "\n",
        "$\\mathbf{y} = [y_1, y_2, ... , y_M]^T$ is a columns vector of true values of the outcome (from data), and $\\hat{\\mathbf{y}}$ is a vector of predicted values.\n",
        "\n",
        "We have:\n",
        "\n",
        "$$\\mathbf{y} \\approx \\hat{\\mathbf{y}} = \\mathbf{X} \\mathbf{w}$$\n",
        "\n",
        "### Loss function\n",
        "\n",
        "$$\\mathcal{L}(\\mathbf{w}) = \\frac{1}{2M}\\sum_{j=1}^M (y_j - \\mathbf{x}_j \\mathbf{w})^2 = \\frac{1}{2M} || \\mathbf{y} - \\mathbf{X} \\mathbf{w}||_2^2,$$\n",
        "\n",
        "where $\\mathbf{x}_j$ is a sample vector (a row of $\\mathbf{X}$) and $(y_j - \\mathbf{x}_j \\mathbf{w})$ is the error between the true and predicted values. The loss function is the average squared errors over $M$ samples.\n",
        "\n",
        "To find $f$, we have to find $\\mathbf{w}$ so that the loss function, $\\mathcal{L}(\\mathbf{w})$, has the lowest value. In other words, to find $\\mathbf{w}$ that minimize the average error between the true and predicted values. We need to find $\\mathbf{w}^*$ as:\n",
        "\n",
        "$$\\mathbf{w}^* = \\arg\\min_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w})$$\n",
        "\n",
        "### Solution of the linear regression problem\n",
        "\n",
        "We can find $\\mathbf{w}$ by solving an optimization problem. Since the loss function is a quadratic function, we find $\\mathbf{w}$ by solving:\n",
        "$$\\frac{\\partial{\\mathcal{L}(\\mathbf{w})}}{\\partial{\\mathbf{w}}} = 0$$\n",
        "\n",
        "Solution:\n",
        "$$\\mathbf{w} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$$\n",
        "\n",
        "If $(\\mathbf{X}^T \\mathbf{X})$ is not invertiable, then we can use the pseudo inverse concept. In this case:\n",
        "$$\\mathbf{w} = (\\mathbf{X}^T \\mathbf{X})^{†} \\mathbf{X}^T \\mathbf{y},$$\n",
        "\n",
        "where $(\\mathbf{X}^T \\mathbf{X})^{†}$ is the pseudo inverse of matrix $(\\mathbf{X}^T \\mathbf{X})$\n",
        "\n",
        "### Numerical optimization\n",
        "\n",
        "Another way to solve the optimization problem is using a numerical method such as gradient descent. We will update the weights using gradient descent algorihtm:\n",
        "\n",
        "$$\\mathbf{w} := \\mathbf{w} - \\alpha \\frac{\\partial{\\mathcal{L}(\\mathbf{w})}}{\\partial{\\mathbf{w}}} = \\mathbf{w} - \\alpha \\nabla_{\\mathbf{w}}\\mathcal{L}(\\mathbf{w})$$\n",
        "\n",
        "We have:\n",
        "\n",
        "$$\\nabla_{\\mathbf{w}}\\mathcal{L}(\\mathbf{w}) = \\frac{1}{M} \\mathbf{X}^T(\\mathbf{X} \\mathbf{w}-\\mathbf{y}) = \\frac{1}{M} \\mathbf{X}^T(\\hat{\\mathbf{y}}-\\mathbf{y})$$"
      ],
      "metadata": {
        "id": "uX36YUNcJYdV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation"
      ],
      "metadata": {
        "id": "6LXNDt6a7zJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.datasets import make_regression \n",
        "\n",
        "# creating the data set\n",
        "X, y = make_regression(n_samples=100, n_features=2, n_targets=1, noise=20, random_state=1)\n",
        "\n",
        "# splitting training and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=1) "
      ],
      "metadata": {
        "id": "37IeWEP674VC"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "sRO6rgVbIzU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc076d2b-a7a0-4ec0-9690-a90cd4218e8a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'W': array([[ 3.03731077],\n",
              "        [31.64575094],\n",
              "        [80.925791  ]])}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "class MyLR1:\n",
        "  \"\"\"\n",
        "  Linear Regression implementation with exact formula\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    self.params = {\n",
        "        'W': None\n",
        "    }\n",
        "\n",
        "  def train(self, X_train, y_train):\n",
        "    y_train = y_train.copy().reshape((-1,1))\n",
        "    assert X_train.shape[0] == y_train.shape[0]\n",
        "\n",
        "    one = np.ones((X_train.shape[0], 1))\n",
        "    # X = np.concatenate((one, X_train), axis = 1)\n",
        "    X = X_train.copy()\n",
        "    X = np.hstack((one, X)) # to include bias\n",
        "\n",
        "    # W =  np.dot(np.linalg.pinv(np.dot(X.T, X)), np.dot(X.T, y_train))\n",
        "    W = np.linalg.pinv(X.T @ X) @ X.T @ y_train\n",
        "    assert W.shape == (X.shape[1],1)\n",
        "    self.params['W'] = W\n",
        "\n",
        "  def predict(self, X_test):\n",
        "    one = np.ones((X_test.shape[0], 1))\n",
        "    X = np.hstack((one, X_test))\n",
        "    y_preds = X @ self.params['W']\n",
        "    return y_preds\n",
        "\n",
        "\n",
        "mylr1 = MyLR1()\n",
        "mylr1.train(X_train, y_train)\n",
        "mylr1.predict(X_test)\n",
        "mylr1.params"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyLR2:\n",
        "\n",
        "  \"\"\"\n",
        "  Linear Regression implementation using gradient descent\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, learning_rate=0.01, n_iter=1000):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.n_iter = n_iter\n",
        "    self.params = {\n",
        "        'W': None,\n",
        "    }\n",
        "\n",
        "  def init_params(self, n_weights):\n",
        "    self.params['W'] = np.zeros((n_weights, 1))\n",
        "\n",
        "\n",
        "  def gradient_descent(self, M, W, X, y):\n",
        "    W -= 1/M * self.learning_rate * (X.T @ (X @ W - y))\n",
        "    return W\n",
        "\n",
        "  def train(self, X_train, y_train):\n",
        "    y = y_train.copy().reshape((-1,1))\n",
        "    assert X_train.shape[0] == y_train.shape[0]\n",
        "\n",
        "    one = np.ones((X_train.shape[0], 1))\n",
        "    X = X_train.copy()\n",
        "    X = np.hstack((one, X)) # to include bias\n",
        "\n",
        "    M, N = X.shape\n",
        "    self.init_params(N)\n",
        "\n",
        "    W = self.params['W']\n",
        "    for i in range(self.n_iter):\n",
        "      W = self.gradient_descent(M, W, X, y)\n",
        "    self.params['W'] = W\n",
        "\n",
        "  def predict(self, X_test):\n",
        "    one = np.ones((X_test.shape[0], 1))\n",
        "    X = np.hstack((one, X_test))\n",
        "    y_preds = X @ self.params['W']\n",
        "    return y_preds\n",
        "\n",
        "mylr2 = MyLR2()\n",
        "mylr2.train(X_train, y_train)\n",
        "mylr2.params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-G9QnfCT6cs",
        "outputId": "5ae88fb0-6fdc-4b50-d4c8-432ffdb02d73"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'W': array([[ 3.09489998],\n",
              "        [31.61578819],\n",
              "        [80.83326253]])}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyLR3:\n",
        "\n",
        "  \"\"\"\n",
        "  Linear Regression implementation using gradient descent but sperating weights and bias\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, learning_rate=0.01, n_iter=1000):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.n_iter = n_iter\n",
        "    self.params = {\n",
        "        'W': None,\n",
        "        'b': None\n",
        "    }\n",
        "\n",
        "  def init_params(self, n_weights):\n",
        "    self.params['W'] = np.zeros((n_weights, 1))\n",
        "    self.params['b'] = 0\n",
        "\n",
        "  def gradient_descent(self, X, y):\n",
        "    M, N = X.shape\n",
        "    W, b = self.params['W'], self.params['b']\n",
        "    for i in range(self.n_iter):\n",
        "      y_pred = X @ W + b\n",
        "      W -= 1/M * self.learning_rate * (X.T @ (y_pred - y))\n",
        "      b -= 1/M * self.learning_rate * np.sum(y_pred-y) # In fact it is them same as b.T @ (y_pred-y) when writing b as a vector\n",
        "    self.params['W'] = W\n",
        "    self.params['b'] = b\n",
        "\n",
        "  def train(self, X_train, y_train):\n",
        "    y = y_train.copy().reshape((-1,1))\n",
        "    X = X_train.copy()\n",
        "    assert X.shape[0] == y.shape[0]\n",
        "\n",
        "    self.init_params(X.shape[1])\n",
        "    self.gradient_descent(X, y)\n",
        "\n",
        "  def predict(self, X_test):\n",
        "      y_preds = np.dot(X_test, self.params[\"W\"]) + self.params[\"b\"]\n",
        "      return y_preds\n",
        "\n",
        "mylr3 = MyLR3()\n",
        "mylr3.train(X_train, y_train)\n",
        "mylr3.params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9AWwMzv34X3",
        "outputId": "ebb3e952-e0fd-466a-d30a-6b24229f875c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'W': array([[31.61578819],\n",
              "        [80.83326253]]), 'b': 3.094899980947451}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using sklearn\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train, y_train)\n",
        "print(lin_reg.coef_)\n",
        "print(lin_reg.intercept_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cN0_UF4UeGT",
        "outputId": "742b941b-2cf1-415f-ef97-3c8e95b03a64"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[31.64575094 80.925791  ]\n",
            "3.037310773722737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare results\n",
        "\n",
        "mse1 = np.mean((mylr1.predict(X_test).flatten() - y_test)**2)\n",
        "mse2 = np.mean((mylr2.predict(X_test).flatten() - y_test)**2)\n",
        "mse3 = np.mean((mylr3.predict(X_test).flatten() - y_test)**2)\n",
        "mse4 = np.mean((lin_reg.predict(X_test) - y_test)**2)\n",
        "print(mse1, mse2, mse3, mse4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYO6tdTF80kN",
        "outputId": "c24364aa-38d5-4351-9d65-b6866b72b65d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "641.7771707649288 642.1429652605785 642.1429652605786 641.7771707649296\n"
          ]
        }
      ]
    }
  ]
}